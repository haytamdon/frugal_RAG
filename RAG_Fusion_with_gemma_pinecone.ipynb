{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClinhMTLT2I3"
      },
      "source": [
        "# Install Ollama on Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvG1JA7l74oX",
        "outputId": "6d3b3f57-a2c6-4087-81e8-764315c4894f"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPgP1sQvT8Ay"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SddmE0PFaK0S",
        "outputId": "51ceef21-cab8-4e40-924d-fe05ac899c03"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pinecone-client pinecone-datasets langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI9OsR1gT6lX",
        "outputId": "34c5031c-da9b-4af1-ed57-d8762c3e7b8a"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBH7W-PnUnKt"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OWy9nLCpT5Aq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.load import dumps, loads\n",
        "import pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YbYEDu-bVHk3"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <your_langsmith_api_key>\n",
        "os.environ['PINECONE_API_KEY'] = <your_pinecone_api_key>\n",
        "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
        "use_serverless = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "aNmBezD2VPg7"
      },
      "outputs": [],
      "source": [
        "# Load blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Gv4W2bopV6uo"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tFx69czNexpo"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "if use_serverless:\n",
        "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "else:\n",
        "    # if not using a starter index, you should specify a pod_type too\n",
        "    spec = PodSpec()\n",
        "# check for and delete index if already exists\n",
        "index_name = 'langchain-retrieval-augmentation-fast'\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "# create a new index\n",
        "pc.create_index(\n",
        "    index_name,\n",
        "    dimension=384,  # dimensionality of GPT4ALLEmbeddings\n",
        "    metric='dotproduct',\n",
        "    spec=spec\n",
        ")\n",
        "# wait for index to be initialized\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNKbXGvHhMRc",
        "outputId": "53144a95-ca0a-4e37-d192-b7819329bae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dGwFqA79rNKs"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "embedding = GPT4AllEmbeddings()\n",
        "vectorstore = Pinecone.from_documents(splits, embedding, index_name = index_name)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tjOUA7H8UsWt"
      },
      "outputs": [],
      "source": [
        "# RAG-Fusion: Related\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "x-YNzwAtVX6U"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "local_llm = \"gemma:7b\"\n",
        "\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "9dFI570oU1UL"
      },
      "outputs": [],
      "source": [
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | llm\n",
        "    | JsonOutputParser()\n",
        "    | (lambda x: list(x.values()))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "GPD2FSrMVrKk"
      },
      "outputs": [],
      "source": [
        "question = \"What is prompt engineering?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Vv3RUjOeWjFC"
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ZKLyFlpkmg",
        "outputId": "e2f86428-e78a-44cb-e258-2c9c938d01bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Define prompt engineering', 'How does prompt engineering work in AI models?', 'What are the benefits of effective prompt engineering?', 'Examples of successful prompt engineering in different applications']\n"
          ]
        }
      ],
      "source": [
        "# Getting the different resulting queries\n",
        "queries = generate_queries.invoke({\"question\": question})\n",
        "print(queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "nRLa_dBvoS0E"
      },
      "outputs": [],
      "source": [
        "# Retrieving the relevent documents for the queries\n",
        "total_retrieved_docs = []\n",
        "for query in queries:\n",
        "    retrieved_docs = retriever.get_relevant_documents(query)\n",
        "    total_retrieved_docs.extend(retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ys1OvwIspi1j"
      },
      "outputs": [],
      "source": [
        "# Ranking the documents\n",
        "docs = reciprocal_rank_fusion(total_retrieved_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2tgTjYJaVA5E",
        "outputId": "9dd2ba31-4726-41e6-a966-d281f4d33eb8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{ \"Prompt Engineering\" : \"Prompt Engineering refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\" }'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": itemgetter(\"docs\"),\n",
        "    \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question, \"docs\": docs})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
