{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNI4FuDaVya"
      },
      "source": [
        "# Install Ollama on Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPfS1ATPUJ1m",
        "outputId": "ca148a41-d50c-4976-fd67-a30f170aae81"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL71xpzbaY8Q"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpwykTbY5dc",
        "outputId": "63460ab9-3f5c-475b-cead-ebb296c0eb57"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYSTPNjaafb"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "o2yDtzAMQwwM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langchain import hub\n",
        "import bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OLwyTSe5Y8n9"
      },
      "outputs": [],
      "source": [
        "# Activate Langsmith tracing\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <langsmith_api_key>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "g2X2r4UoZNzI"
      },
      "outputs": [],
      "source": [
        "# Choose model in Ollama\n",
        "local_llm = \"mistral:latest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cPSgNBbBQzcG"
      },
      "outputs": [],
      "source": [
        "# Load\n",
        "url = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZUA5S9-OQ2nR"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VO6YZQLKQ42X"
      },
      "outputs": [],
      "source": [
        "# Embed and index\n",
        "embedding = GPT4AllEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8VRXTBE-Q7Mx"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embedding,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HNpUMqTpQ-PX"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "# RAG prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "question = \"What is prompt engineering?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZOjRsTVGa7np"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "rag_chain = prompt | llm | JsonOutputParser()\n",
        "result = rag_chain.invoke({\"context\": docs, \"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoo3YlgpUOO7",
        "outputId": "5f3fd7df-ba7e-4fed-a96f-9a3ced99ed7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/',\n",
              " 'title': \"Prompt Engineering | Lil'Log\",\n",
              " 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.',\n",
              " 'language': 'en'}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
