{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e078m2mO5xoB"
      },
      "source": [
        "# Install Ollama on Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJJcPO1oc8gq",
        "outputId": "d0a02a88-7cdd-42cd-dab6-539a5dd6355d"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcPOPhVY529z"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBgFTKjMdm_T",
        "outputId": "f1ed62ba-0588-4fa8-c637-683d501effeb"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efFoVmFN56mR"
      },
      "source": [
        "# Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EJyOzf3-eKgN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "koqy57e1u-xx"
      },
      "outputs": [],
      "source": [
        "# Activate Langsmith tracing\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <langsmith_api_key>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBxWghM6AHB"
      },
      "source": [
        "# Loading document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CY3EOhaCjWNT"
      },
      "outputs": [],
      "source": [
        "# Load blog\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I_JA8Kl04rHo"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwNWyLLE6CmG"
      },
      "source": [
        "# Load vector storage Chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X0qnP1hKdo37"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "embedding = GPT4AllEmbeddings()\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=embedding)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8wEP5kW5Vlg"
      },
      "source": [
        "# Multi Query\n",
        "In multiquerying, we make the LLM come up with multiple versions of the question that would be more appropriate for similarity searches on the vector embedding space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pjlWaikEeEi2"
      },
      "outputs": [],
      "source": [
        "# Multi Query: Different Perspectives\n",
        "\n",
        "template = \"\"\"You are an AI language model assistant. You are to generate five\n",
        "different versions of the given user question. By generating multiple perspectives\n",
        "on the user question, your goal is to help the user overcome some of the limitations\n",
        "of the distance-based similarity search. Provide these alternative questions\n",
        "separated by commas. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Local LLM\n",
        "local_llm = \"llama2:13b\"\n",
        "\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | llm\n",
        "    | JsonOutputParser()\n",
        "    | (lambda x: list(x.keys()) + list(x.values()))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hzzPn20qeG9d"
      },
      "outputs": [],
      "source": [
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is prompt engineering?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\":question})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OebGmggWeJat",
        "outputId": "cea889ab-08d2-4517-8869-838db7588ac0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{ \"Prompt engineering refers to the process of designing and crafting input statements, or prompts, for large language models to generate specific and desired outputs. It involves understanding the model\\'s capabilities and limitations, as well as the nuances of human language and communication.\"\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# RAG\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain,\n",
        "    \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({\"question\":question})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
