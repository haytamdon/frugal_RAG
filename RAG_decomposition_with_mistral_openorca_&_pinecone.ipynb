{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT7vH2kjvumv",
        "outputId": "07bcf6f3-c5b8-4718-d6bd-cb17579c3286"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGp-ShDYvxSE",
        "outputId": "160ed06f-273c-4ce6-eb34-967b2a661d44"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pinecone-client pinecone-datasets langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOBb0MzrvywD",
        "outputId": "a8d6f2a4-abe9-4e23-9560-91253fd090f2"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NvDtRqtJv0Mj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.load import dumps, loads\n",
        "import pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from langchain import hub\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d07llWuzwKJ5"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
        "use_serverless = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b2j7GvSIxUNJ"
      },
      "outputs": [],
      "source": [
        "# Load blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=( \"https://en.wikipedia.org/wiki/Albert_Einstein\",\n",
        "                \"https://www.nobelprize.org/prizes/physics/1921/einstein/biographical/\",\n",
        "                \"https://www.britannica.com/biography/Albert-Einstein/General-relativity-and-teaching-career\",\n",
        "                \"https://www.space.com/15524-albert-einstein.html\"),\n",
        "    # bs_kwargs=dict(\n",
        "    #     parse_only=bs4.SoupStrainer(\n",
        "    #         class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "    #     )\n",
        "    # ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wSv5cU69xUla"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ0GpLRpxePf",
        "outputId": "0a6c491b-4275-4169-c665-d0899076e9b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "932"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y2UnGP5qxbXv"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "if use_serverless:\n",
        "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "else:\n",
        "    # if not using a starter index, you should specify a pod_type too\n",
        "    spec = PodSpec()\n",
        "# check for and delete index if already exists\n",
        "index_name = 'einstein'\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "# create a new index\n",
        "pc.create_index(\n",
        "    index_name,\n",
        "    dimension=384,  # dimensionality of GPT4ALLEmbeddings\n",
        "    metric='dotproduct',\n",
        "    spec=spec\n",
        ")\n",
        "# wait for index to be initialized\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzKKSWnsx7uL",
        "outputId": "7d74d091-4419-4022-8f54-72fec13eed7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhaDCaf7yU-A",
        "outputId": "2edb8835-e250-45c1-c23c-14d570ee0f5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 45.9M/45.9M [00:00<00:00, 64.3MiB/s]\n",
            "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 551MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "embedding = GPT4AllEmbeddings()\n",
        "vectorstore = Pinecone.from_documents(splits, embedding, index_name = index_name)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzZK6BbByW4M",
        "outputId": "be97de08-db05-4f3e-d71e-01c8281eb29d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IJqSzq_NyZBw"
      },
      "outputs": [],
      "source": [
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of 3 sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate 3 search queries related to: {question} \\n\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2MToVnhu3CBR",
        "outputId": "5b33ab8e-f714-42f1-91c0-fd7c72f79afb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n  \"1\": \"What was Albert Einstein\\'s Theory of Relativity?\",\\n  \"2\": \"Who is Albert Einstein?\",\\n  \"3\": \"What did Albert Einstein contribute to modern physics?\"\\n}<|im_end|>'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LLM\n",
        "local_llm = \"mistral-openorca\"\n",
        "\n",
        "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0.75)\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = (  prompt_decomposition\n",
        "                                    | llm\n",
        "                                    | StrOutputParser()\n",
        "                                    # | (lambda x: x.values())\n",
        "                                  )\n",
        "\n",
        "# Run\n",
        "question = \"What is Albert Einstein most famous for\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4WIdaZYo4SiG"
      },
      "outputs": [],
      "source": [
        "questions_formatted = questions.split(\"\\n\")[1:4]\n",
        "questions_final = [q.split(\":\")[1].strip().split(\",\")[0][1:-1] for q in questions_formatted]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6XciD3EBrG7",
        "outputId": "c9ad040c-a7c9-42f6-d67e-3eb4b7962c3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"What was Albert Einstein's Theory of Relativity?\",\n",
              " 'Who is Albert Einstein?',\n",
              " 'What did Albert Einstein contribute to modern physics?']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gtzS-0TWGbx2"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Here is the question you need to answer:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5DB25V5PGck8"
      },
      "outputs": [],
      "source": [
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions_final:\n",
        "\n",
        "    rag_chain = (\n",
        "    {   \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | decomposition_prompt\n",
        "    | llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK_cyYmuN5BH",
        "outputId": "c27488fa-5830-402d-8335-e5030502f7c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answers': [{'score': 1,\n",
              "   'content': \"Albert Einstein contributed significantly to modern physics by developing the theory of relativity. His theories revolutionized our understanding of space, time, and gravity. The special theory of relativity focuses on the relationship between space and time in constant velocity systems, while the general theory of relativity explains the effects of gravity on objects in motion. Einstein's mass-energy equivalence formula (E=mc^2) further changed our comprehension of energy and matter.\"}]}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json.loads(answer[:-10])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
