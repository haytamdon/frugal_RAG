{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMVgQTzIGxel",
        "outputId": "f3defed9-4401-4f95-d59e-f31b212b848e"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ1ustLNRVZz",
        "outputId": "cb3e094e-24dc-4272-8c56-db0b29a1e466"
      },
      "outputs": [],
      "source": [
        "!pip install -qU pinecone-client pinecone-datasets langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_K3qLrbRW0K",
        "outputId": "95f69514-635a-4b50-fcf0-f2503c1108ab"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKD5K5X3STC0",
        "outputId": "a520f0ca-e083-4a7b-f52d-9d30657081f8"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eSU0UQc7RZJy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from operator import itemgetter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from langchain_community.embeddings import OllamaEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mXxDvFEcTEAt"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "# In the following two lines either replace the function with \n",
        "# your API keys or save them in the secrets section in google \n",
        "# colab\n",
        "os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "pinecone_api_key = os.environ['PINECONE_API_KEY']\n",
        "use_serverless = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-hGkvoLRbPH",
        "outputId": "a27d0ed6-950a-4dcf-cb9f-429baaf92f05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ],
      "source": [
        "# Loading Wikipedia articles about Boeing, in this \n",
        "# case we load 24 Boeing related articles\n",
        "docs = WikipediaLoader(query=\"Boeing\").load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p0UzNRYJYiZa"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3p53o0u0B8n",
        "outputId": "334be1f3-64d0-4a6a-b24e-a1c43ec0acb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We use mxbai embeddings (state of the art embedding as of \n",
        "# March 2024)\n",
        "# As of March 2024, this model archives SOTA performance for \n",
        "# Bert-large sized models on the MTEB. It outperforms commercial models \n",
        "# like OpenAIs text-embedding-3-large model and matches the performance \n",
        "# of model 20x its size.\n",
        "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "len(embeddings.embed_query(\"test query\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lbmD_J4YYnSJ"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "if use_serverless:\n",
        "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "else:\n",
        "    # if not using a starter index, you should specify a pod_type too\n",
        "    spec = PodSpec()\n",
        "# check for and delete index if already exists\n",
        "index_name = 'boeing-db'\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "# create a new index\n",
        "pc.create_index(\n",
        "    index_name,\n",
        "    dimension=1024,  # dimensionality of mxbai-embed-large\n",
        "    metric='dotproduct',\n",
        "    spec=spec\n",
        ")\n",
        "# wait for index to be initialized\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8__VYxVYvzc",
        "outputId": "0088fa6a-61c5-4d92-9c5f-744b87e5c33a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zLkDiLT8Yzkc"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
        "vectorstore = Pinecone.from_documents(splits, embeddings, index_name = index_name)\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5rp_pyK14di",
        "outputId": "f451a453-bb92-4f9f-fc59-665624180bda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 413}},\n",
              " 'total_vector_count': 413}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fLS0mna92J4h"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"what can the members of The Police do?\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "    },\n",
        "]\n",
        "# We now transform these to example messages\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot_prompt,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MPqGqXDi4R56"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "local_llm = \"mistral-openorca\"\n",
        "\n",
        "llm = ChatOllama(model=local_llm, temperature=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vmOKHj3T3rgB",
        "outputId": "af504ba0-e987-4c27-ed00-a227bb988d4d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'which Boeing aircraft is highly regarded?'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_queries_step_back = (prompt |\n",
        "                              llm |\n",
        "                              StrOutputParser() |\n",
        "                              (lambda x: x[:-10]))\n",
        "question = \"Out of all boeing aircraft which one could be considered their best work\"\n",
        "generate_queries_step_back.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "I7nNjMoy4ppJ",
        "outputId": "fc37b025-335a-4fd9-bf6a-4ab95831f195"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The question of which Boeing aircraft is the \"best\" is subjective and depends on various factors, such as a person\\'s preference, industry standards, and the specific needs of the user. However, some models are widely regarded as successful and innovative designs, including the Boeing 787 Dreamliner and the Boeing 747 Jumbo Jet.\\n\\nThe 787 Dreamliner is known for its advanced materials, fuel efficiency, and passenger comfort features. It was designed to be more environmentally friendly, with a focus on reducing emissions and noise levels. The 787 has also received praise for its large windows, mood lighting, and improved air quality systems that contribute to a better in-flight experience for passengers.\\n\\nThe 747 Jumbo Jet revolutionized the aviation industry upon its introduction in 1969. It was the first wide-body aircraft, allowing airlines to transport more passengers and cargo at once while also offering greater comfort with its spacious interior layouts. The 747\\'s unique design, including its hump-shaped upper deck that houses the cockpit and extra seating, has become iconic in popular culture.\\n\\nBoth aircraft have had a significant impact on the airline industry, and they showcase Boeing\\'s commitment to innovation, efficiency, and passenger comfort.'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Response prompt\n",
        "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\"\"\"\n",
        "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        # Retrieve context using the normal question\n",
        "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
        "        # Retrieve context using the step-back question\n",
        "        \"step_back_context\": generate_queries_step_back | retriever,\n",
        "        # Pass on the question\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "    }\n",
        "    | response_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x[:-10])\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": question})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
